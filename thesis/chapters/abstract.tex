Humans are able to learn to understand and process the distribution of space, and one of the initial tasks of Artificial Intelligence has been to show machines the relationships between space and the objects that appear in it. Humans naturally combine vision and textual information to acquire spatial relationships among objects, and when reading a text, we are able to mentally depict the spatial relationships that may appear in it. Thus, the visual differences between images depicting "a person sits and a dog stands" and "a person stands and a dog sits" are obvious for humans, but still not clear for automatic systems. In this project, we propose to build grounded Neural Language models that are able to perform this kind of spatial reasoning. Neural Language models (LM) have shown impressive capabilities on many NLP tasks but, despite their success, they have been criticized for their lack of meaning. Vision-and-Language models (VLM), trained jointly on text and image or video data, have been offered as a response to such criticisms, but recent work has shown that these models struggle to ground spatial concepts properly. In the project we propose to build spatially-aware language models that ground spatial concepts in images. We propose to use a variety of methods that involve the creation of synthetic datasets specially focused on spatial reasoning capabilities, as well as the use of multi-task learning. We expect the new models to improve the state of the art in spatial reasoning. Code is released at \url{https://github.com/juletx/spatial-reasoning} and models are released at \url{https://huggingface.co/juletxara}.

\paragraph{Keywords:} Artificial Intelligence, Deep Learning, Natural Language Processing, Computer Vision, Grounding, Visual Reasoning, Compositional Reasoning, Spatial Reasoning