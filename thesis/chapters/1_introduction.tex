Neural \textbf{Language Models} (LM) have shown \textbf{impressive capabilities} on many Natural Language Processing (NLP) tasks \cite{wang2019superglue, brown2020language, chowdhery2022palm}. LMs are pretrained on large corpora in order for them to learn universal language representations, which are beneficial for downstream NLP tasks and can avoid training a new model from scratch. The \textbf{pretrained models are fine-tuned in specific downstream tasks}, using annotated data that is orders of magnitude smaller than the text used in the pretraining phase. Following this transfer learning methodology, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English \cite{bommasani2021opportunities, wang2019superglue}.

Despite the impressive results of LMs for different language-related tasks, many authors criticize them for their \textbf{lack of meaning} \cite{bender2020climbing, bender2021dangers}. In their opinion, language models trained exclusively on language are unable to learn meaning. Those authors suggest that \textbf{grounding is one of the key elements to bring human-like language understanding}. However, language grounding is a very broad area that covers a great diversity of techniques, modalities and concepts. 

In this project, we will focus on grounding LMs for \textbf{compositional and spatial reasoning}. We choose compositional and spatial reasoning because they are crucial to how humans organize the mental space and make sense of the physical world, and therefore fundamental for a grounded theory of semantics \cite{levinson2003space}. However, \textbf{spatial reasoning} has been found to be \textbf{particularly challenging for current models} \cite{akula2020words}. Another work has found that these models are not as skilled at visio-linguistic \textbf{compositional reasoning} as we might have hoped \cite{thrush2022winoground}. That is, compositional and spatial reasoning are much more challenging than capturing properties of individual entities.

\textbf{Vision Language Models} (VLMs), which are trained jointly on text and image, have been proposed as a general solution to the lack of grounding in language models \cite{lu2019vilbert, tan2020lxmert, ramesh2022hierarchical, saharia2022photorealistic}. Vision-language pre-training aims to improve performance of downstream vision and language tasks by \textbf{pretraining the model on many image-text pairs}. These pre-trained models can then be fine-tuned on each downstream task. VLMs have been fine-tuned in tasks that require grounding spatial concepts, such as VQA \cite{antol2015vqa} or NLVR2 \cite{suhr2018corpus}.

With the objective of \textbf{evaluating spatial relations}, a recent work provides new unified datasets \cite{liu2022things}. As the objective of such work is to evaluate whether VLMs learn more spatial commonsense than LMs, the datasets are purely textual, so they do not provide any means to ground spatial concepts. Interestingly, authors find that VLMs, and more concretely text-to-image systems, perform much better than text-only LMs. Still, they show that VLMs \textbf{struggle to ground spatial concepts properly}.

Large generative \textbf{text-to-image diffusion models}, like DALLE-2 \cite{ramesh2022hierarchical}, IMAGEN \cite{saharia2022photorealistic} and Stable Diffusion \cite{rombach2021highresolution}, are able to generate stunning images. They are known to possess some visual-reasoning skills \cite{cho2022dall}. However, a recent work \cite{liu2022compositional} has shown that they \textbf{struggle to understand the composition of some concepts}, such as confusing the attributes and relations of different objects. They propose a new method, where an image is generated by composing a set of diffusion models, with each of them modelling a certain component of the image. Another work \cite{anonymous2023trainingfree} proposes manipulating cross-attention representations to address three challenging phenomena in Stable Diffusion \cite{rombach2021highresolution}: attribute leakage, interchanged attributes and missing objects.

There are several \textbf{works that try to ground language models to spatial relations}. For example, \cite{bagherinezhad2016elephants, elazar2019large} focus on the acquired commonsense knowledge of models about object scales (e.g. is a person bigger than an ant?). However, they ask about generic object scale relations, without providing any context. Some other authors \cite{collell2018acquiring, elu2021inferring} work on implicit and explicit spatial relations of objects, given some descriptive texts. The proposed benchmark datasets are designed for object bounding box generation.

\section{Objectives} \label{sec:objectives}

Despite the impressive performance of pretrained vision and language models (VLMs) on a wide variety of multimodal tasks, they remain poorly understood. One important question is to what extent such models are able to conduct unimodal and multimodal \textbf{compositional reasoning} and \textbf{spatial reasoning}. For example, the visual differences between images depicting "a person sits and a dog stands" and "a person stands and a dog sits" are really obvious for humans, but still not clear for current state-of-the-art VLMs. To perform well on tasks where compositional and spatial reasoning is required, the models do not only need a proper encoding of text and images but also to be able to \textbf{ground meaning across the two modalities}.

Thus the main objective of the project is to \textbf{evaluate grounded language models for compositional and spatial reasoning}. The goal is to investigate ways to acquire grounded representation for compositional and spatial reasoning. In that sense, we will evaluate state-of-the-art pre-trained and fine-tuned VLMs to understand their grounding level. Towards this goal, this project will focus on using the latest advances in deep-learning techniques, and pre-trained LMs for effective zero-shot transfer learning. We have defined these objectives for this project:

\textbf{Improve the state of the art in compositional reasoning}. Winoground \cite{thrush2022winoground} is a dataset for evaluating the ability of VLMs to conduct visio-linguistic compositional reasoning. The original Winoground paper included zero-shot experiments with many pre-trained SOTA systems, and they concluded that none of them does much better than chance. In this work, we will extend the previous experiments with new pre-trained models and models that are fine-tuned for specific tasks such as image-text retrieval and visual reasoning. Our objective is to improve results and to find out which models and tasks help most for compositional reasoning.

\textbf{Perform zero-shot experiments in spatial reasoning.} Visual Spatial Reasoning \cite{liu2022visual} is a dataset for investigating VLMs capabilities in recognising 65 types of spatial relationships in natural text-image pairs. VSR authors \cite{liu2022visual} train and test three popular VLMs: VisualBERT \cite{li2019visualbert}, LXMERT \cite{tan2020lxmert}, and
ViLT \cite{kim2021vilt}. They conclude that there is still a large gap between model and human performance. We will extend these experiments with zero-shot experiments with ViLT \cite{kim2021vilt} and BLIP \cite{li2022blip} models fine-tuned on NLVR2 dataset. Our aim is to measure how this affects performance on VSR.

\textbf{Investigate the use of synthetic datasets to overcome the lack of annotated datasets}. As to avoid the scarcity of multimodal datasets that explicitly describe compositional and spatial relations, we propose to automatically construct synthetic datasets. These datasets could be used in the future to train existing language models in a self-supervised way, with the final aim of obtaining spatially grounded language models. In particular, we will explore the viability of three alternatives with evaluations: text-to-image generation, image captioning and image retrieval. These techniques could be combined to produce synthetic datasets in the future.

\textbf{Large generative text-to-image VLMs} are known to generate great images from text descriptions, and they possess some visual reasoning skills. We will use Stable Diffusion \cite{rombach2021highresolution} to obtain realistic images with entities that are arranged following certain spatial relations. We will do a manual evaluation of the generated images to measure their quality and decide if they can be used to create synthetic datasets.

\textbf{Image captioning models} could provide extra information about the images to the models, that is not included in the original captions. Most large datasets contain captions directly obtained from the image descriptions on the internet. Those descriptions are often not very good. Image captioning models can be used to get decent captions much faster than with human annotation. We will do a qualitative evaluation of a small sample of captions to know how good they are.

\textbf{Image retrieval systems} could be used to retrieve images of interest from large image datasets. Images can be retrieved using similarity scores between caption and image embeddings. This can be combined with captioning to improve original captions. We will evaluate a small sample of the retrieved images to decide their viability for synthetic dataset generation.

\section{Structure} \label{sec:structure}

This section provides an overview of the next chapters in this work: \ref{cha:background} \nameref{cha:background}, \ref{cha:winoground_zero_shot} \nameref{cha:winoground_zero_shot}, \ref{cha:vsr_zero_shot} \nameref{cha:vsr_zero_shot}, \ref{cha:synthetic_dataset} \nameref{cha:synthetic_dataset}, \ref{cha:conclusions} \nameref{cha:conclusions} and \ref{cha:future_work} \nameref{cha:future_work}.

First, \cref{cha:background} contains the background knowledge used in this project's development. This chapter includes two main sections: Multimodal Models and Visual Reasoning Datasets. \cref{sec:multimodal_models} explains the types of models that are related to this work. \cref{sec:visual_reasoning_datasets} includes synthetic and natural visual reasoning datasets and the datasets that we chose for this work.

Second, \cref{cha:winoground_zero_shot} explains how we achieve the first objective, improving SOTA in compositional reasoning. First, it describes the Winoground \cite{thrush2022winoground} dataset (\cref{sec:winoground_dataset}) and explains the metrics used for evaluation. We also describe a series of previous and new experiments performed over the Winoground dataset using state-of-the-art vision and language models (\cref{sec:winoground_experiments_results}). The Winoground dataset does not contain a training split, and therefore the experiments are conducted in a zero-shot fashion, where the models are trained on different datasets, and tested on Winoground.

Next, \cref{cha:vsr_zero_shot} is related to the second objective, performing zero-shot experiments on spatial reasoning. We focus on Visual Spatial Reasoning \cite{liu2022visual}, a dataset for spatial reasoning (\cref{sec:vsr_dataset}). Unlike Winoground, VSR contains training and validation splits and can be used to train models (\cref{sec:vsr_splits}). However, we mainly focus on zero-shot experiments with models that are trained on other datasets. We explain previous and new experiments we performed and the results we obtained in VSR (\cref{sec:vsr_experiments_results}).

Then, \cref{cha:synthetic_dataset} describes the third objective, synthetic dataset creation. We investigate three different options for synthetic dataset generation: Text-to-Image Generation (\ref{sec:text_to_image_generation}), Image Captioning (\ref{sec:image_captioning}) and Image Retrieval (\ref{sec:image_retrieval}). These experiments also allow us to gain more insight into the dataset and the tested models.

Finally, \cref{cha:conclusions,cha:future_work} provide an overview of the main contributions, conclusions and future work areas for further research. This chapter provides an overview of future work areas for further research. We propose four ideas for synthetic dataset generation (\cref{sec:synthetic_dataset}): explicit verbalization (\ref{sec:explicit_verbalization}), text-to-image (\ref{sec:text-to-image}), image-to-image (\ref{sec:image-to-image}) and image captioning and retrieval (\ref{sec:image_captioning_retrieval}). We also include some ideas for extending current datasets to be multilingual (\cref{sec:multilingual_datasets}).