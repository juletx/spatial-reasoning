Neural \textbf{Language Models} (LM) have shown \textbf{impressive capabilities} on many Natural Language Processing (NLP) tasks \cite{wang2019superglue, brown2020language, chowdhery2022palm}. LMs are pretrained on large corpora in order for them to learn universal language representations, which are beneficial for downstream NLP tasks and can avoid training a new model from scratch. The \textbf{pretrained models are fine-tuned in specific downstream tasks}, using annotated data that is orders of magnitude smaller than the text used in the pretraining phase. Following this transfer learning methodology, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English \cite{bommasani2021opportunities, wang2019superglue}.

Despite the impressive results of LMs for different language-related tasks, many authors criticize them for their \textbf{lack of meaning} \cite{bender2020climbing, bender2021dangers}. In their opinion, language models trained exclusively on language are unable to learn meaning. Those authors suggest that \textbf{grounding is one of the key elements to bring human-like language understanding}. However, language grounding is a very broad area that covers a great diversity of techniques, modalities and concepts. 

In this project, we will focus on \textbf{compositional and spatial reasoning}. Spatial reasoning consists on \textbf{grounding LMs with spatial concepts}. We choose compositional and spatial reasoning because they are fundamental capabilities for both humans and LMs. Spatial relations are crucial to how humans organize the mental space and make sense of the physical world, and therefore fundamental for a grounded theory of semantics \cite{levinson2003space}. However, spatial reasoning has been found to be \textbf{particularly challenging for current models} \cite{akula2020words}. That is, spatial reasoning is much more challenging than capturing properties of individual entities.

\textbf{Vision Language Models} (VLMs), which are trained jointly on text and image, have been proposed as a general solution to the lack of grounding in language models \cite{lu2019vilbert, tan2020lxmert, ramesh2022hierarchical, saharia2022photorealistic}. Vision-language pre-training aims to improve performance of downstream vision and language tasks by \textbf{pretraining the model on many image-text pairs}. These pre-trained models can then be fine-tuned on each downstream task. VLMs have been fine-tuned in tasks that require grounding spatial concepts, such as VQA \cite{antol2015vqa} or NLVR2 \cite{suhr2018corpus}.

With the objective of \textbf{evaluating spatial relations}, a recent work provides new unified datasets \cite{liu2022things}. As the objective of such work is to evaluate whether VLMs learn more spatial commonsense than LMs, the datasets are purely textual, so they do not provide any means to ground spatial concepts. Interestingly, authors find that VLMs, and more concretely text-to-image systems, perform much better than text-only LMs. Still, they show that VLMs \textbf{struggle to ground spatial concepts properly}.

Large generative \textbf{text-to-image diffusion models}, like DALLE-2 \cite{ramesh2022hierarchical} and IMAGEN \cite{saharia2022photorealistic}, are able to generate stunning images. They are known to possess some visual-reasoning skills \cite{cho2022dall}. However, a recent work \cite{liu2022compositional} has shown that they \textbf{struggle to understand the composition of some concepts}, such as confusing the attributes and relations of different objects. They propose a new method, where an image is generated by composing a set of diffusion models, with each of them modelling a certain component of the image. Another work \cite{anonymous2023trainingfree} proposes manipulating cross-attention representations to address three challenging phenomena in Stable Diffusion \cite{rombach2021highresolution}: attribute leakage, interchanged attributes and missing objects.

There are several \textbf{works that try to ground language models to spatial relations}. For example, \cite{bagherinezhad2016elephants, elazar2019large} focus on the acquired commonsense knowledge of models about object scales (e.g. is a person bigger than an ant?). However, they ask about generic object scale relations, without providing any context. Some other authors \cite{collell2018acquiring, elu2021inferring} work on implicit and explicit spatial relations of objects, given some descriptive texts. The proposed benchmark datasets are designed for object bounding box generation.

\section{Objectives} \label{sec:objectives}

Despite the impressive performance of pretrained vision and language models (VLMs) on a wide variety of multimodal tasks, they remain poorly understood. One important question is to what extent such models are able to conduct unimodal and multimodal \textbf{compositional reasoning} and \textbf{spatial reasoning}. For example, the visual differences between images depicting "a person sits and a dog stands" and "a person stands and a dog sits" are really obvious for humans, but still not clear for current state-of-the-art VLMs. To perform well on tasks where compositional and spatial reasoning is required, the models do not only need a proper encoding of text and images but also to be able to \textbf{ground meaning across the two modalities}.

Thus the main objective of the project is to \textbf{learn grounded language models for compositional and spatial reasoning}. The goal is to investigate ways to acquire grounded representation for compositional and spatial reasoning. In that sense, we will define suitable ways to incorporate spatial information into pre-trained vision and language models. Towards this goal, this project will focus on using the latest advances in deep-learning techniques, and pre-trained LMs for effective zero-shot transfer learning. We have defined two objectives for this project:

\textbf{Improve the state of the art in compositional and spatial reasoning}. The final goal is to apply the findings learnt from previous objectives to improve the state-of-the-art in multiple datasets. We plan to evaluate our models with at least two vision and language datasets. The first one is the Winoground dataset \cite{thrush2022winoground}, which presents a novel task for evaluating the ability of vision and language models to conduct visio-linguistic compositional reasoning. The second one is the VSR benchmark \cite{liu2022visual} for investigating VLMs capabilities in recognising 65 types of spatial relationships in natural text-image pairs.

\textbf{Investigate the use of synthetic datasets to overcome the lack of annotated datasets for spatial grounding}. As to avoid the scarcity of multimodal datasets that explicitly describe compositional and spatial relations, we propose to automatically construct synthetic datasets. These datasets can later be used to train existing language models in a self-supervised way, with the final aid of obtaining spatially grounded language models. In particular, we propose three alternatives that can be combined to produce the synthetic datasets:
\begin{enumerate}
    \item \textbf{Large generative text-to-image VLMs}, which are known to obey spatial relations as described in the text, to obtain realistic images with entities that are arranged following certain spatial relations.
    \item \textbf{Image captioning models} could provide extra information about the images to the models, that is not included in the original captions. Most large datasets contain captions directly obtained from the image descriptions on the internet. Those descriptions are often not very good. Image captioning models can be used to obtain decent captions much faster than with human annotation.
    \item \textbf{Image retrieval systems} could be used to retrieve images of interest from large image datasets. Images can be retrieved using similarity scores between caption and image embeddings. This can be combined with captioning to improve original captions.
\end{enumerate}

\section{Structure} \label{sec:structure}

This section provides an overview of the next chapters in this work: \ref{cha:background} \nameref{cha:background}, \ref{cha:winoground_zero_shot} \nameref{cha:winoground_zero_shot}, \ref{cha:synthetic_dataset} \nameref{cha:synthetic_dataset}, \ref{cha:visual_spatial_reasoning} \nameref{cha:visual_spatial_reasoning}, \ref{cha:conclusions} \nameref{cha:conclusions} and \ref{cha:future_work} \nameref{cha:future_work}

First, \cref{cha:background} contains the background knowledge used in this project's development. This chapter includes two main sections: Multimodal Models and Visual Reasoning Datasets. \cref{sec:multimodal_models} explains the types of models that are related to this work. \cref{sec:visual_reasoning_datasets} includes synthetic and natural visual reasoning datasets and the datasets that we chose for this work.

Second, \cref{cha:winoground_zero_shot} describes the Winoground \cite{thrush2022winoground} dataset (\cref{sec:winoground_dataset}) and explains the metrics used for evaluation. We also describe a series of previous and new experiments performed over the Winoground dataset using state-of-the-art vision and language models (\cref{sec:winoground_experiments_results}). The Winoground dataset does not contain a training split, and therefore the experiments are conducted in a zero-shot fashion, where the models are trained on different datasets, and tested on Winoground.

Then, \cref{cha:synthetic_dataset} includes more experiments that were performed on Winoground to gain more insight into the dataset and the tested models. These experiments include Text-to-Image Generation (\ref{sec:text_to_image_generation}), Image Captioning (\ref{sec:image_captioning}) and Image Retrieval (\ref{sec:image_retrieval}).

Next, \cref{cha:visual_spatial_reasoning} introduces the Visual Spatial Reasoning (VSR) \cite{liu2022visual} dataset (\cref{sec:vsr_dataset}) and the different data splits (\cref{sec:vsr_splits}) that are used for evaluation. We also explain previous experiments and new experiments we performed and the results we obtained in VSR (\cref{sec:vsr_experiments_results}).

Later, \cref{cha:conclusions} provides an overview of the main contributions and conclusions of this work.

Finally, \cref{cha:future_work} provides an overview of future work areas for further research. \cref{sec:zero_shot} includes ideas for improving the state-of-the-art. We also propose four ideas for synthetic dataset generation (\cref{sec:synthetic_dataset}). Finally, we include some ideas for extending current datasets to be multilingual (\cref{sec:multilingual_datasets}).