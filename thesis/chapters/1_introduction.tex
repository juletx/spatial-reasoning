This chapter is an introduction of the master thesis and includes background, objectives, research questions and contributions.

\section{Background}

This section includes information about grounding Language Models (\ref{sec:language_models}) and Vision Language Models (\ref{sec:vision_language_models}).

\subsection{Language Models} \label{sec:language_models}

Neural Language Models (LM) have shown \textbf{impressive capabilities} on many Natural Language Processing (NLP) tasks \cite{wang2019superglue, brown2020language, chowdhery2022palm}. LMs are pretrained on large corpora in order for them to learn universal language representations, which are beneficial for downstream NLP tasks and can avoid training a new model from scratch. The \textbf{pretrained models are fine-tuned in specific downstream tasks}, using annotated data that is orders of magnitude smaller than the text used in the pretraining phase. Following this transfer learning methodology, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English \cite{bommasani2021opportunities, wang2019superglue}.

Despite the impressive results of LMs for different language-related tasks, many authors criticize them for their \textbf{lack of meaning} \cite{bender2020climbing, bender2021dangers}. In their opinion, language models trained exclusively on language are unable to learn meaning. Those authors suggest that \textbf{grounding is one of the key elements to bring human-like language understanding}. However, language grounding is a very broad area that covers a great diversity of techniques, modalities and concepts. 

In this project, we will focus on \textbf{compositional and spatial reasoning}. Spatial reasoning consists on \textbf{grounding LMs with spatial concepts}. We choose spatial reasoning because it is one of the most fundamental capabilities for both humans and LMs. Such relations are crucial to how humans organize the mental space and make sense of the physical world, and therefore fundamental for a grounded theory of semantics \cite{levinson2003space}. However, spatial reasoning has been found to be \textbf{particularly challenging for current models} \cite{akula2020words}. That is, spatial reasoning is much more challenging than capturing properties of individual entities.

\subsection{Vision Language Models} \label{sec:vision_language_models}

Vision Language Models (VLM), which are trained jointly on text and image, have been proposed as a general solution to the lack of grounding in language models \cite{lu2019vilbert, tan2020lxmert, ramesh2022hierarchical, saharia2022photorealistic}. Vision-language pre-training aims to improve performance of downstream vision and language tasks by \textbf{pretraining the model on many image-text pairs}. These pre-trained models can then be fine-tuned on each downstream task. VLMs have been fine-tuned in tasks that require grounding spatial concepts, such as VQA \cite{antol2015vqa} or NLVR2 \cite{suhr2018corpus}.

With the objective of \textbf{evaluating spatial relations}, a recent work provides new unified datasets \cite{liu2022things}. As the objective of such work is to evaluate whether VLMs learn more spatial commonsense than LMs, the datasets are purely textual, so they do not provide any means to ground spatial concepts. Interestingly, authors find that VLMs, and more concretely text-to-image systems, perform much better than text-only LMs. Still, they show that VLMs \textbf{struggle to ground spatial concepts properly}.

Large generative \textbf{text-to-image diffusion models}, like DALLE-2 \cite{ramesh2022hierarchical} and IMAGEN \cite{saharia2022photorealistic}, are able to generate stunning images. They are known to possess some visual-reasoning skills \cite{cho2022dall}. However, a recent work \cite{liu2022compositional} has shown that they \textbf{struggle to understand the composition of some concepts}, such as confusing the attributes and relations of different objects. They propose a new method, where an image is generated by composing a set of diffusion models, with each of them modeling a certain component of the image. Another work \cite{anonymous2023trainingfree} proposes manipulating cross-attention representations to address three challenging phenomena in Stable Diffusion \cite{rombach2021highresolution}: attribute leakage, interchanged attributes and missing objects.

There are several \textbf{works that try to ground language models to spatial relations}. For example, \cite{bagherinezhad2016elephants, elazar2019large} focus on the acquired commonsense knowledge of models about object scales, e.g. do they know that a person is bigger than an ant? However, they ask about generic object scale relations, without providing any context. Some other authors \cite{collell2018acquiring, elu2021inferring} work on implicit and explicit spatial relations of objects, given some descriptive texts. The proposed benchmark datasets are designed for object bounding box generation.

\section{Objectives}

Despite the impressive performance of pretrained vision and language models (VLMs) on a wide variety of multimodal tasks, they remain poorly understood. One important question is to what extent such models are able to conduct unimodal and multimodal \textbf{compositional reasoning} and \textbf{spatial reasoning}. For example, the visual differences between images depicting "a person sits and a dog stands" and "a person stands and a dog sits" are clamorously obvious for humans, but still not clear for current state-of-the-art VLMs. To perform well on tasks where compositional and spatial reasoning is required, the models do not only need a proper encoding of text and images but also to be able to \textbf{ground meaning across the two modalities}.

Thus the main objective of the project is to \textbf{learn language models for spatial reasoning via the grounding of LMs with spatial concepts and relations}. One of the main goals of the project is to investigate ways to acquire grounded representation for spatial reasoning. In that sense, we will define suitable ways to incorporate spatial information into pre-trained vision and language models. Towards this goal, this project will focus on using the latest advances in deep-learning techniques, pre-trained LMs for effective zero and few-shot transfer learning.

We have defined the following specific objectives in the scope of spatial reasoning:

\begin{enumerate}
    \item \textbf{Investigate the use of synthetic datasets to overcome the lack of annotated datasets for spatial grounding}. As to avoid the scarcity of multimodal datasets that explicitly describe spatial relations, we propose to automatically construct synthetic datasets on spatial relations and use them to train existing language models in a self-supervised way, with the final aid of obtaining spatially grounded language models. In particular, we propose two alternatives to produce the synthetic datasets:
    \begin{enumerate}
        \item \textbf{Explicit verbalization} of spatial relations in images. Given an image in an existing dataset, we propose to use an object detector to identify the entities in the images, as well as hand-designed verbalization templates to automatically generate textual descriptions of the spatial relations among them.
        \item \textbf{Using large generative VLMs}, which are known to obey spatial relations as described in the text, to obtain realistic images with entities that are arranged following certain spatial relations.
    \end{enumerate}
    \item \textbf{Investigate the use of multi-tasking and multi-sourcing to improve generalization properties}. In a multi-task training paradigm, the model is forced to learn more than one task simultaneously, therefore improving its generalization capabilities. We will investigate multi-task settings to combine the verbalized dataset, the images produced by the generative VLMs, as well as traditional training data to obtain spatial-aware language models.
    \item \textbf{Improve zero-shot and few-shot generalization of VLM models} to obtain effective models in small data regimes of the spatial reasoning domain without the necessity of explicitly annotating big quantities of spatial relations.
    \item \textbf{Improve the state of the art in compositional and spatial reasoning}. Improve the state of the art in compositional and spatial reasoning. The final goal is to apply the findings learnt from previous objectives to improve the state-of-the-art in multiple datasets. We plan to evaluate our models at least on two vision and language datasets. The first one is the Winoground dataset \cite{thrush2022winoground}, which presents a novel task for evaluating the ability of vision and language models to conduct visio-linguistic compositional reasoning. The second one is the VSR benchmark \cite{liu2022visual} for investigating VLMs capabilities in recognising 65 types of spatial relationships in natural text-image pairs.
\end{enumerate}

\section{Research Questions}

Research Tasks (RT) and Questions (RQ) are based on the objectives from the previous section.

\textbf{RT0. Prepare the research scenario}. The initial task is related to \textbf{gathering corpora, exploring different datasets}, \textbf{Language Models} (LM) and \textbf{building a baseline prototype}. We have already identified some important datasets on spatial reasoning but we will check if there is any new appropriate dataset to evaluate our models. At the same time, we will examine and reimplement (if needed) state-of-the-art systems in order to further understand the task to be solved. This leads us to the following research questions: \textbf{RQ0.A) Are the available datasets appropriate to evaluate the spatial abilities of current LMs?} \textbf{RQ0.B) Which is the best pre-trained LMs for spatial reasoning?} We will conduct a quantitative and qualitative analysis of the existing text-only LMs and vision-language LMs in order to 1) measure the appropriateness of probing evaluations of the datasets and 2) explain the limitations of different types of pre-trained LMs.

\textbf{RT2: Perform synthetic data generation using generative models to learn spatial grounding}. We will focus on using large generative VLMs to construct high quality synthetic images that depict a fixed set of spatial relations. In that sense, we want to answer the following research questions. \textbf{RQ2.A) Which is the right way to make explicit the implicit information encoded in generative VLMs?} \textbf{RQ2.B) Can we improve the state-of-the-art of vision and language models in tasks that require spatial reasoning?}

\textbf{RT3: Perform multi-task and multi-source learning in few-shot settings}. In this task we will focus on finding ways of applying multi-task learning using multiple sources of information in order to force LMs to ground spatial relations into text without the necessity of explicitly annotating big quantities of spatial relations. This leads us to the following research questions: \textbf{RQ3.A) What kind of tasks and information sources are relevant to learn spatial information effectively?} \textbf{RQ3.B) What is the best way to combine the task in a multi-task setting?} \textbf{RQ3.C) Can we effectively minimize annotated data to obtain state-of-the-art results in tasks that require spatial reasoning?}

\section{Contributions}

This section provides an overview of the main contributions of this work.

\textbf{Improve the state of the art in compositional reasoning}. The original Winoground paper included zero-shot experiments with many pre-trained SOTA systems, and they concluded that, surprisingly, none of them does much better than chance \cite{thrush2022winoground}. In this work, we extended the previous experiments with new models that obtained better results than those reported in the original paper. In previous experiments, only pre-trained models are tested. We extend this by testing some models that are fine-tuned for specific tasks such as image-text retrieval and visual reasoning. We compare pre-trained versions with fine-tuned versions of the same models and find out that fine-tuning helps.

\textbf{Investigate image generation using generative models.} With the aim of evaluating the compositional ability of diffusion models, we used the state-of-the-art Stable Diffusion model \cite{rombach2021highresolution} to generate images 9 images for each Winoground caption. The general conclusion is that Stable Diffusion is not good at this task. Most of the generated images do not match the captions. Therefore, using a diffusion model for data augmentation might not be robust enough. It would require generating many images to get the correct ones, and manual filtering to discard the wrong images.

\textbf{Investigate image captioning for compositional reasoning.} With the aim of obtaining more insight into the Winoground examples, we decided to test image captioning. We used OFA \cite{wang2022unifying} and BLIP \cite{li2022blip} models of different sizes to generate captions for all Winoground images. The general conclusion is that most captions are quite good. They are very different from the original ones, but they describe the images correctly. They provide extra information about the images to the models, that is not included in the original captions.

\textbf{Investigate image retrieval for compositional reasoning.} We used CLIP retrieval\footnote{\url{https://github.com/rom1504/clip-retrieval}} to retrieve images from LAION-5B \cite{schuhmann2022laionb} dataset. We used Winoground captions and images to get similar images. This system could be used to increase the size of our dataset. We could retrieve many similar images for our captions. We could also change the captions to retrieve images with different objects. Nevertheless, this would also require some filtering because there are many wrong images.

\textbf{Perform zero-shot experiments in spatial reasoning.} VSR authors \cite{liu2022visual} train and test three popular VLMs: VisualBERT \cite{li2019visualbert}, LXMERT \cite{tan2020lxmert}, and
ViLT \cite{kim2021vilt}. They conclude that there is still a large gap between model and human performance. We extend these experiments and evaluate ViLT \cite{kim2021vilt} and BLIP \cite{li2022blip} models fine-tuned on NLVR2. We show that performance drops a lot.

\section{Structure}

This section provides an overview of the next chapters in this work: \ref{cha:2_background} \nameref{cha:2_background}, \ref{cha:4_winoground} \nameref{cha:4_winoground}, \ref{cha:5_experiments} \nameref{cha:5_experiments}, \ref{cha:6_visual_spatial_reasoning} \nameref{cha:6_visual_spatial_reasoning} and \ref{cha:7_conclusions_future_work} \nameref{cha:7_conclusions_future_work}.

First, \cref{cha:2_background} contains the background knowledge used in this project's development. This chapter includes two main sections: Multimodal Models and Visual Reasoning Datasets. \cref{sec:multimodal_models} explains the types of models that are related to this work. \cref{sec:visual_reasoning_datasets} includes synthetic and natural visual reasoning datasets and the datasets that we chose for this work.

Second, \cref{cha:4_winoground} describes the Winoground \cite{thrush2022winoground} dataset (\cref{sec:winoground_dataset}) and explains the metrics used for evaluation. We also describe a series of previous and new experiments performed over the Winoground dataset using state-of-the-art vision and language models (\cref{sec:winoground_experiments_results}). The Winoground dataset does not contain a training split, and therefore the experiments are conducted in a zero-shot fashion, where the models are trained on different datasets, and tested on Winoground.

Then, \cref{cha:5_experiments} includes more experiments that were performed on Winoground to gain more insight into the dataset and the tested models. These experiments include Text-to-Image Generation (\ref{text_to_image_generation}), Image Captioning (\ref{image_captioning}) and Image Retrieval (\ref{image_retrieval}).

Next, \cref{cha:6_visual_spatial_reasoning} introduces the Visual Spatial Reasoning (VSR) \cite{liu2022visual} dataset (\cref{sec:vsr_dataset}) and the different data splits (\cref{sec:vsr_splits}) that are used for evaluation. We also explain previous experiments and new experiments we performed and the results we obtained in VSR (\cref{sec:vsr_experiments_results}).

Finally, \cref{cha:7_conclusions_future_work} includes conclusions \cref{sec:conclusions} and future work areas for further research \cref{sec:future_work}.