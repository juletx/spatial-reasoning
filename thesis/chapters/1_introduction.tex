This chapter is an introduction of the master thesis and includes background, objectives and research questions.

\section{Objectives}

Despite the impressive performance of pretrained vision and language models (VLMs) on a wide variety of multimodal tasks, they remain poorly understood. One important question is to what extent such models are able to conduct unimodal and multimodal compositional reasoning and spatial reasoning. For example, the visual differences between images depicting "a person sits and a dog stands" and "a person stands and a dog sits" are clamorously obvious for humans, but still not clear for current state-of-the-art VLMs. To perform well on tasks where compositional and spatial reasoning is required, the models do not only need a proper encoding of text and images, but also to be able to \textbf{ground meaning across the two modalities} (spatial grounding).

Thus the main objective of the project is to \textbf{learn language models for spatial reasoning via the grounding of LMs with spatial concepts and relations}. One of the main goals of the project is to investigate ways to acquire grounded representation for spatial reasoning. In that sense, we will define suitable ways to incorporate spatial information into pre-trained vision and language models. Towards this goal, this project will focus on using the latest advances in deep-learning techniques, pre-trained LMs for effective zero and few-shot transfer learning.

We have defined the following specific objectives in the scope of spatial reasoning:

\begin{enumerate}
    \item \textbf{Investigate the use of synthetic datasets to overcome the lack of annotated datasets for spatial grounding}. As to avoid the scarcity of multimodal datasets that explicitly describe spatial relations, we propose to automatically construct synthetic datasets on spatial relations and use them to train existing language models in a self-supervised way, with the final aid of obtaining spatially grounded language models. In particular, we propose two alternatives to produce the synthetic datasets:
    \begin{enumerate}
        \item \textbf{Explicit verbalization} of spatial relations in images. Given an image in an existing dataset, we propose to use an object detector to identify the entities in the images, as well as hand-designed verbalization templates to automatically generate textual descriptions of the spatial relations among them.
        \item \textbf{Using large generative VLMs}, which are known to obey spatial relations as described in the text, to obtain realistic images with entities that are arranged following certain spatial relations.
    \end{enumerate}
    \item \textbf{Investigate the use of multi-tasking and multi-sourcing to improve generalization properties}. In a multi-task training paradigm, the model is forced to learn more than one task simultaneously, therefore improving its generalization capabilities. We will investigate multi-task settings to combine the verbalized dataset, the images produced by the generative VLMs, as well as traditional training data to obtain spatial-aware language models.
    \item \textbf{Improve zero-shot and few-shot generalization of VLM models} to obtain effective models in small data regimes of the spatial reasoning domain without the necessity of explicitly annotating big quantities of spatial relations.
    \item \textbf{Improve the state of the art in spatial reasoning}. Improve the state of the art in spatial reasoning. The final goal is to apply the findings learnt from previous objectives to improve the state-of-the-art in multiple datasets. We plan to evaluate our models at least on two vision and language datasets. The first one is the Winoground dataset \cite{thrush2022winoground}, which presents a novel task for evaluating the ability of vision and language models to conduct visio-linguistic compositional reasoning. The second one is the VSR benchmark \cite{liu2022visual} for investigating VLMs capabilities in recognising 65 types of spatial relationships in natural text-image pairs.
\end{enumerate}

\section{Research Questions}

Research Tasks (RT) and Questions (RQ) are based on the objectives from the previous section.

\textbf{RT0. Prepare the research scenario}. The initial task is related to \textbf{gathering corpora, exploring different datasets}, \textbf{Language Models} (LM) and \textbf{building a baseline prototype}. We have already identified some important datasets on spatial reasoning but we will check if there is any new appropriate dataset to evaluate our models. At the same time, we will examine and reimplement (if needed) state-of-the-art systems in order to further understand the task to be solved. This leads us to the following research questions: \textbf{RQ0.A) Are the available datasets appropriate to evaluate the spatial abilities of current LMs?} \textbf{RQ0.B) Which is the best pre-trained LMs for spatial reasoning?} We will conduct a quantitative and qualitative analysis of the existing text-only LMs and vision-language LMs in order to 1) measure the appropriateness of probing evaluations of the datasets and 2) explain the limitations of different types of pre-trained LMs.

\textbf{RT2: Perform synthetic data generation using generative models to learn spatial grounding}. We will focus on using large generative VLMs to construct high quality synthetic images that depict a fixed set of spatial relations. In that sense, we want to answer the following research questions. \textbf{RQ2.A) Which is the right way to make explicit the implicit information encoded in generative VLMs?} \textbf{RQ2.B) Can we improve the state-of-the-art of vision and language models in tasks that require spatial reasoning?}

\textbf{RT3: Perform multi-task and multi-source learning in few-shot settings}. In this task we will focus on finding ways of applying multi-task learning using multiple sources of information in order to force LMs to ground spatial relations into text without the necessity of explicitly annotating big quantities of spatial relations. This leads us to the following research questions: \textbf{RQ3.A) What kind of tasks and information sources are relevant to learn spatial information effectively?} \textbf{RQ3.B) What is the best way to combine the task in a multi-task setting?} \textbf{RQ3.C) Can we effectively minimize annotated data to obtain state-of-the-art results in tasks that require spatial reasoning?}