This chapter is an introduction of the master thesis and includes background, objectives, research questions and contributions.

\section{Background}

This section includes information about grounding Language Models (\ref{sec:language_models}) and Vision Language Models (\ref{sec:vision_language_models}).

\subsection{Language Models} \label{sec:language_models}

Neural Language Models (LM) have shown \textbf{impressive capabilities} on many Natural Language Processing (NLP) tasks \cite{wang2019superglue, brown2020language, chowdhery2022palm}. LMs are pretrained on large corpora in order for them to learn universal language representations, which are beneficial for downstream NLP tasks and can avoid training a new model from scratch. The \textbf{pretrained models are fine-tuned in specific downstream tasks}, using annotated data that is orders of magnitude smaller than the text used in the pretraining phase. Following this transfer learning methodology, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English \cite{bommasani2021opportunities, wang2019superglue}.

Despite the impressive results of LMs for different language-related tasks, many authors criticize them for their \textbf{lack of meaning} \cite{bender2020climbing, bender2021dangers}. In their opinion, language models trained exclusively on language are unable to learn meaning. Those authors suggest that \textbf{grounding is one of the key elements to bring human-like language understanding}. However, language grounding is a very broad area that covers a great diversity of techniques, modalities and concepts. 

In this project, we will focus on \textbf{compositional and spatial reasoning}. Spatial reasoning consists on \textbf{grounding LMs with spatial concepts}. We choose spatial reasoning because it is one of the most fundamental capabilities for both humans and LMs. Such relations are crucial to how humans organize the mental space and make sense of the physical world, and therefore fundamental for a grounded theory of semantics \cite{levinson2003space}. However, spatial reasoning has been found to be \textbf{particularly challenging for current models} \cite{akula2020words}. That is, spatial reasoning is much more challenging than capturing properties of individual entities.

\subsection{Vision Language Models} \label{sec:vision_language_models}

Vision Language Models (VLM), which are trained jointly on text and image, have been proposed as a general solution to the lack of grounding in language models \cite{lu2019vilbert, tan2020lxmert, ramesh2022hierarchical, saharia2022photorealistic}. Vision-language pre-training aims to improve performance of downstream vision and language tasks by \textbf{pretraining the model on many image-text pairs}. These pre-trained models can then be fine-tuned on each downstream task. VLMs have been fine-tuned in tasks that require grounding spatial concepts, such as VQA \cite{antol2015vqa} or NLVR2 \cite{suhr2018corpus}.

With the objective of \textbf{evaluating spatial relations}, a recent work provides new unified datasets \cite{liu2022things}. As the objective of such work is to evaluate whether VLMs learn more spatial commonsense than LMs, the datasets are purely textual, so they do not provide any means to ground spatial concepts. Interestingly, authors find that VLMs, and more concretely text-to-image systems, perform much better than text-only LMs. Still, they show that VLMs \textbf{struggle to ground spatial concepts properly}.

Large generative \textbf{text-to-image diffusion models}, like DALLE-2 \cite{ramesh2022hierarchical} and IMAGEN \cite{saharia2022photorealistic}, are able to generate stunning images. They are are known to possess some visual-reasoning skills \cite{cho2022dall}. However, a recent work \cite{liu2022compositional} has shown that they \textbf{struggle to understand the composition of some concepts}, such as confusing the attributes and relations of different objects. They propose a new method, where an image is generated by composing a set of diffusion models, with each of them modeling a certain component of the image. Another work \cite{anonymous2023trainingfree} proposes manipulating cross-attention representations to address three challenging phenomena in Stable Diffusion \cite{rombach2021highresolution}: attribute leakage, interchanged attributes and missing objects.

There are several \textbf{works that try to ground language models to spatial relations}. For example, \cite{bagherinezhad2016elephants, elazar2019large} focus on the acquired commonsense knowledge of models about object scales, e.g. do they know that a person is bigger than an ant? However, they ask about generic object scale relations, without providing any context. Some other authors \cite{collell2018acquiring, elu2021inferring} work on implicit and explicit spatial relations of objects, given some descriptive texts. The proposed benchmark datasets are designed for object bounding box generation.

\section{Objectives}

Despite the impressive performance of pretrained vision and language models (VLMs) on a wide variety of multimodal tasks, they remain poorly understood. One important question is to what extent such models are able to conduct unimodal and multimodal compositional reasoning and spatial reasoning. For example, the visual differences between images depicting "a person sits and a dog stands" and "a person stands and a dog sits" are clamorously obvious for humans, but still not clear for current state-of-the-art VLMs. To perform well on tasks where compositional and spatial reasoning is required, the models do not only need a proper encoding of text and images, but also to be able to \textbf{ground meaning across the two modalities} (spatial grounding).

Thus the main objective of the project is to \textbf{learn language models for spatial reasoning via the grounding of LMs with spatial concepts and relations}. One of the main goals of the project is to investigate ways to acquire grounded representation for spatial reasoning. In that sense, we will define suitable ways to incorporate spatial information into pre-trained vision and language models. Towards this goal, this project will focus on using the latest advances in deep-learning techniques, pre-trained LMs for effective zero and few-shot transfer learning.

We have defined the following specific objectives in the scope of spatial reasoning:

\begin{enumerate}
    \item \textbf{Investigate the use of synthetic datasets to overcome the lack of annotated datasets for spatial grounding}. As to avoid the scarcity of multimodal datasets that explicitly describe spatial relations, we propose to automatically construct synthetic datasets on spatial relations and use them to train existing language models in a self-supervised way, with the final aid of obtaining spatially grounded language models. In particular, we propose two alternatives to produce the synthetic datasets:
    \begin{enumerate}
        \item \textbf{Explicit verbalization} of spatial relations in images. Given an image in an existing dataset, we propose to use an object detector to identify the entities in the images, as well as hand-designed verbalization templates to automatically generate textual descriptions of the spatial relations among them.
        \item \textbf{Using large generative VLMs}, which are known to obey spatial relations as described in the text, to obtain realistic images with entities that are arranged following certain spatial relations.
    \end{enumerate}
    \item \textbf{Investigate the use of multi-tasking and multi-sourcing to improve generalization properties}. In a multi-task training paradigm, the model is forced to learn more than one task simultaneously, therefore improving its generalization capabilities. We will investigate multi-task settings to combine the verbalized dataset, the images produced by the generative VLMs, as well as traditional training data to obtain spatial-aware language models.
    \item \textbf{Improve zero-shot and few-shot generalization of VLM models} to obtain effective models in small data regimes of the spatial reasoning domain without the necessity of explicitly annotating big quantities of spatial relations.
    \item \textbf{Improve the state of the art in spatial reasoning}. Improve the state of the art in spatial reasoning. The final goal is to apply the findings learnt from previous objectives to improve the state-of-the-art in multiple datasets. We plan to evaluate our models at least on two vision and language datasets. The first one is the Winoground dataset \cite{thrush2022winoground}, which presents a novel task for evaluating the ability of vision and language models to conduct visio-linguistic compositional reasoning. The second one is the VSR benchmark \cite{liu2022visual} for investigating VLMs capabilities in recognising 65 types of spatial relationships in natural text-image pairs.
\end{enumerate}

\section{Research Questions}

Research Tasks (RT) and Questions (RQ) are based on the objectives from the previous section.

\textbf{RT0. Prepare the research scenario}. The initial task is related to \textbf{gathering corpora, exploring different datasets}, \textbf{Language Models} (LM) and \textbf{building a baseline prototype}. We have already identified some important datasets on spatial reasoning but we will check if there is any new appropriate dataset to evaluate our models. At the same time, we will examine and reimplement (if needed) state-of-the-art systems in order to further understand the task to be solved. This leads us to the following research questions: \textbf{RQ0.A) Are the available datasets appropriate to evaluate the spatial abilities of current LMs?} \textbf{RQ0.B) Which is the best pre-trained LMs for spatial reasoning?} We will conduct a quantitative and qualitative analysis of the existing text-only LMs and vision-language LMs in order to 1) measure the appropriateness of probing evaluations of the datasets and 2) explain the limitations of different types of pre-trained LMs.

\textbf{RT2: Perform synthetic data generation using generative models to learn spatial grounding}. We will focus on using large generative VLMs to construct high quality synthetic images that depict a fixed set of spatial relations. In that sense, we want to answer the following research questions. \textbf{RQ2.A) Which is the right way to make explicit the implicit information encoded in generative VLMs?} \textbf{RQ2.B) Can we improve the state-of-the-art of vision and language models in tasks that require spatial reasoning?}

\textbf{RT3: Perform multi-task and multi-source learning in few-shot settings}. In this task we will focus on finding ways of applying multi-task learning using multiple sources of information in order to force LMs to ground spatial relations into text without the necessity of explicitly annotating big quantities of spatial relations. This leads us to the following research questions: \textbf{RQ3.A) What kind of tasks and information sources are relevant to learn spatial information effectively?} \textbf{RQ3.B) What is the best way to combine the task in a multi-task setting?} \textbf{RQ3.C) Can we effectively minimize annotated data to obtain state-of-the-art results in tasks that require spatial reasoning?}

\section{Contributions}

\section{Structure}