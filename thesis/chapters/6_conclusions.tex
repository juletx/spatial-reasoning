In this chapter, we provide an overview of the main contributions and conclusions of this work. We managed to accomplish all the objectives of this work. First, we improved the state-of-the-art in compositional reasoning. Next, we performed some zero-shot experiments on spatial reasoning. Finally, we explored three alternatives for synthetic dataset creation: text-to-image generation, image captioning and image retrieval.

\textbf{Improve the state-of-the-art in compositional reasoning}. The original Winoground paper included zero-shot experiments with many pre-trained SOTA systems, and they concluded that, surprisingly, none of them does much better than chance \cite{thrush2022winoground}. In this work, we extended the previous experiments with new models that obtained better results than those reported in the original paper. In previous experiments, only pre-trained models are tested. We extend this by testing some models that are fine-tuned for specific tasks such as image-text retrieval and visual reasoning. We compare pre-trained versions with fine-tuned versions of the same models and find out that fine-tuning helps a lot. Our best scores are much better than previous models, ~10\% in text score, ~4\% in image score and ~7\% in group score. However, even the best model is still \textbf{far from human performance} in text, image and group scores. There is still a ~40\% gap in text scores, and ~64\% in image and group scores.

\textbf{Perform zero-shot experiments in spatial reasoning.} VSR authors \cite{liu2022visual} train and test three popular VLMs: VisualBERT \cite{li2019visualbert}, LXMERT \cite{tan2020lxmert}, and
ViLT \cite{kim2021vilt}. They conclude that there is still a large gap between model and human performance. We extend these experiments and evaluate ViLT \cite{kim2021vilt} and BLIP \cite{li2022blip} models fine-tuned on NLVR2. We show that performance drops a lot when evaluating ViLT as zero-shot. This is understandable because spatial relations are only a small part of NLVR2. Moreover, NLVR2 examples contain two images, and VSR examples have only one image. To evaluate on VSR, we need to pass the same image twice, or change the caption to mention one of the images. If we want to improve VSR results in the future, we will need to fine-tune better VLMs such as BLIP directly on VSR.

\textbf{Investigate text-to-image generative models for synthetic dataset creation.} With the aim of evaluating the compositional ability of diffusion models, we used the state-of-the-art Stable Diffusion model \cite{rombach2021highresolution} to generate images for each Winoground caption. We did a manual evaluation of the generated images to measure their quality and decide if they can be used to create synthetic datasets. The general conclusion is that Stable Diffusion is not good at this task. Most of the generated images do not match the captions. Therefore, using a diffusion model for data augmentation might not be robust enough. It would require generating many images to get the correct ones, and manual filtering to discard the wrong images.

\textbf{Investigate image captioning for synthetic dataset creation.} We used OFA \cite{wang2022unifying} and BLIP \cite{li2022blip} models of different sizes to generate captions for all Winoground images. A manual evaluation on a small sample showed that the quality of the generated captions were high. They are very different from the original ones, but they describe the images correctly. They provide extra information about the images to the models, that is not included in the original captions. Captioning could be applied for generating automatic descriptions of images much faster than humans. This would reduce the work needed to create training datasets targeted for compositional or spatial reasoning.

\textbf{Investigate image retrieval for synthetic dataset creation.} We used CLIP retrieval to retrieve images from LAION-5B \cite{schuhmann2022laionb} dataset. We used Winoground captions and images to get similar images. This system could be used to increase the size of our dataset. We could retrieve many similar images for our captions. We could also change the captions to retrieve images with different objects. This would also require some filtering because there are many wrong images.