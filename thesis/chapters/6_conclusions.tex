This chapter provides an overview of the main contributions and conclusions of this work.

\textbf{Improve the state of the art in compositional reasoning}. The original Winoground paper included zero-shot experiments with many pre-trained SOTA systems, and they concluded that, surprisingly, none of them does much better than chance \cite{thrush2022winoground}. In this work, we extended the previous experiments with new models that obtained better results than those reported in the original paper. In previous experiments, only pre-trained models are tested. We extend this by testing some models that are fine-tuned for specific tasks such as image-text retrieval and visual reasoning. We compare pre-trained versions with fine-tuned versions of the same models and find out that fine-tuning helps.

\textbf{Investigate text-to-image generative models for synthetic dataset creation.} With the aim of evaluating the compositional ability of diffusion models, we used the state-of-the-art Stable Diffusion model \cite{rombach2021highresolution} to generate images 9 images for each Winoground caption. The general conclusion is that Stable Diffusion is not good at this task. Most of the generated images do not match the captions. Therefore, using a diffusion model for data augmentation might not be robust enough. It would require generating many images to get the correct ones, and manual filtering to discard the wrong images.

\textbf{Investigate image captioning for synthetic dataset creation.} With the aim of obtaining more insight into the Winoground examples, we decided to test image captioning. We used OFA \cite{wang2022unifying} and BLIP \cite{li2022blip} models of different sizes to generate captions for all Winoground images. The general conclusion is that most captions are quite good. They are very different from the original ones, but they describe the images correctly. They provide extra information about the images to the models, that is not included in the original captions.

\textbf{Investigate image retrieval for synthetic dataset creation.} We used CLIP retrieval\footnote{\url{https://github.com/rom1504/clip-retrieval}} to retrieve images from LAION-5B \cite{schuhmann2022laionb} dataset. We used Winoground captions and images to get similar images. This system could be used to increase the size of our dataset. We could retrieve many similar images for our captions. We could also change the captions to retrieve images with different objects. Nevertheless, this would also require some filtering because there are many wrong images.

\textbf{Perform zero-shot experiments in spatial reasoning.} VSR authors \cite{liu2022visual} train and test three popular VLMs: VisualBERT \cite{li2019visualbert}, LXMERT \cite{tan2020lxmert}, and
ViLT \cite{kim2021vilt}. They conclude that there is still a large gap between model and human performance. We extend these experiments and evaluate ViLT \cite{kim2021vilt} and BLIP \cite{li2022blip} models fine-tuned on NLVR2. We show that performance drops a lot.