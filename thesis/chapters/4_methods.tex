This chapter explains the methods we used for evaluation in each dataset. This includes model configurations and other settings.

\section{Winoground}

We introduce baseline models and our models in \Cref{sec:wino_models} and then provide an overview of all the models.

\subsection{Models} \label{sec:wino_models}

\paragraph{Baselines.}
Winoground authors \cite{thrush2022winoground} evaluate various configurations of the following multimodal transformers: CLIP \cite{radford2021clip}, FLAVA \cite{singh2022flava}, LXMERT \cite{tan2020lxmert}, UniT \cite{hu2021unit}, UNITER \cite{chen2020uniter}, VILLA \cite{gan2020villa}, VinVL \cite{zhang2021vinvl}, ViLT \cite{kim2021vilt}, VisualBERT \cite{li2019visualbert} and ViLBERT \cite{lu2019vilbert}. They also evaluate several configurations of two types of RNN-based models: VSE++ \cite{faghri2018vse} and VSRN \cite{li2019vsrn}.

\paragraph{Ours.}
We evaluate various configurations of the following multimodal transformers: OFA \cite{wang2022unifying}, BLIP \cite{li2022blip}, CLIP \cite{radford2021clip}, FLAVA \cite{singh2022flava} and ViLT \cite{kim2021vilt}. OFA and BLIP were not included in the baseline evaluation. The other models were already included but we test more configurations. For example, we test models that are finetuned on Flickr30k, COCO, NLVR2 and VSR. We try different model sizes when they are available.

\paragraph{Overview.}
We provide a high-level overview of the differences between the models in \cref{tab:model-types} which includes pretraining datasets, architecture, and attention mechanisms between the modalities. We omit datasets that were only used to train backbones. We exclude the language embedding from this table as every model uses a pretrained BERT tokenizer, except CLIP, VSE++, and VSRN. The pretraining datasets include COCO \cite{lin2014microsoft}, Visual Genome (VG) \cite{krishna2016visual}, Conceptual Captions (CC) \cite{sharma2018conceptual}, SBU Captions \cite{ordonez2011im2text}, Flickr30k \cite{young2014image}, VQA 2.0 \cite{goyal2017making}, VCR \cite{zellers2019recognition}, NLVR2 \cite{suhr2017corpus}, SNLI-VE \cite{xie2018visual}, QNLI \cite{rajpurkar2016squad}, MLNI-mm \cite{williams2017broad}, QQP \cite{QQPDataset}, Localized Narratives (LN) \cite{pont-tuset2020localized-narratives}, Wikipedia Image Text (WIT) \cite{srinivasan2021wit}, Conceptual Captions 12M (CC 12M) \cite{changpinyo2021conceptual12m}, Red Caps (RC) \cite{desai2021redcaps}, YFCC100M \cite{thomee2016yfcc100m}, SST-2 \cite{Socher2013RecursiveDM}, and LAION \cite{schuhmann2021laion}. CLIP uses their own dataset for pretraining.

\begin{table}[ht]
    \centering
    \small
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{l|lr|l|l}
    \toprule
    Model & Datasets & \# Images, Captions & Architecture & Attention \\\midrule
    VinVL \cite{zhang2021vinvl}  & VQA, GQA, VG-QA, COCO, Flickr30k, CC, SBU & 1.89, 4.87 & single-stream & merged \\
    UNITER \cite{chen2020uniter}  & COCO, VG, CC, SBU & 4.20, 9.58 & single-stream  &  merged \\
    ViLLA \cite{gan2020villa} & COCO, VG, CC, SBU  & 4.20, 9.58 & single-stream  &  merged \\
    VisualBERT \cite{li2019visualbert}& COCO, NVLR2 & 0.30, 0.52  & single-stream  & merged \\
    ViLT \cite{kim2021vilt}  & COCO, VG, SBU, CC & 4.10, 9.85 & single-stream  & merged \\
    LXMERT \cite{tan2020lxmert}  & COCO, VG & 0.18, 9.18 & dual-stream & modality-specific, co-attn, merged \\
    ViLBERT \cite{lu2019vilbert}  & CC & 3.30, 3.30 & dual-stream  & modality-specific, co-attn, merged \\
    UniT \cite{hu2021unit} & COCO detect., VG detect., VQAv2, SNLI-VE QNLI, MNLI-mm, QQP, SST-2 & 0.69, 1.91 & dual-stream & modality-specific, merged\\
    FLAVA $_{ITM}$ \cite{singh2022flava}  & COCO, SBU, LN, CC, VG, WIT, CC 12M, RC, YFCC100M & 70.00, 70.00 & dual-stream & modality-specific, merged \\
    FLAVA $_{Contrastive}$ \cite{singh2022flava}  & COCO, SBU, LN, CC, VG, WIT, CC 12M, RC, YFCC100M & 70.00, 70.00 & dual-stream & modality-specific \\
    CLIP \cite{radford2021clip}  & $-$ & 400.00, 400.00 & dual-stream & modality-specific \\
    \midrule
    OFA \cite{wang2022unifying} &  CC 12M, CC 3M, SBU, COCO, VG-Cap  &  20.00, 20.00 &   single-stream & modality-specific, merged \\
    BLIP$_{ITM}$ 14M \cite{li2022blip} &  COCO, VG, SBU, CC, CC 12M  &  14.00, 15.00 &   dual-stream & modality-specific, merged \\
    BLIP$_{ITC}$ 14M \cite{li2022blip} &  COCO, VG, SBU, CC, CC 12M & 14.00, 15.00 &   dual-stream &         modality-specific \\
    BLIP$_{ITM}$ 129M \cite{li2022blip} & COCO, VG, SBU, CC, CC 12M, LAION & 129.00,   130.00 &   dual-stream & modality-specific, merged \\
    BLIP$_{ITC}$ 129M \cite{li2022blip} & COCO, VG, SBU, CC, CC 12M, LAION & 129.00,   130.00 &   dual-stream &         modality-specific \\
    \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{A high-level overview of the differences between the models we evaluate by the pretraining datasets, architecture, and attention mechanisms between the modalities.}
    \label{tab:model-types}
\end{table}



\section{Visual Spatial Reasoning}

We introduce baseline models and our models in \Cref{sec:vsr_models}.

\subsection{Models}\label{sec:vsr_models}

\paragraph{Baselines.} VSR authors \cite{liu2022visual} test three popular VLMs: VisualBERT \cite{li2019visualbert}, 
LXMERT \cite{tan2020lxmert}, and
ViLT \cite{kim2021vilt}. All three models are stacked Transformers \cite{vaswani2017attention} that take image and text pairs as input. The difference mainly lies in how or whether they encode position information of objects. Checkpoints are saved every 100 iterations and the best checkpoint on the dev set is used for testing. All models are run three times using three random seeds.

\paragraph{Ours.} We first test the same baseline models. We also evaluate a ViLT model that has only been finetuned on NLVR2. We evaluate BLIP \cite{li2022blip} trained on VSR and NLVR2.

