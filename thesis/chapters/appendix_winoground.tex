This appendix shows extra Winoground discussion results of previous and our experiments. We include correlation analysis between caption perplexity and model scores (\cref{sec:caption_perplexity}). \cref{sec:dataset_size} includes correlation analysis between pretraining dataset size and model scores.

\section{Caption Complexity} \label{sec:caption_perplexity}

\paragraph{Previous.}

\cref{tab:perplexity-and-length-correlations-baseline} shows on the left correlations between \textbf{GPT-2 caption perplexity} and model scores. Winoground authors found that there is typically a weak correlation between models assigning an image-caption pair a higher score and a caption having low perplexity \cite{thrush2022winoground}.

\cref{tab:perplexity-and-length-correlations-baseline} shows on the right that \textbf{caption length} and lower model performance significantly correlate for the best models, even though the correlation is reversed for humans. The examples with the shortest captions are also the least compositional \cite{thrush2022winoground}.

\begin{table}[ht]
\centering
\begin{adjustbox}{max width=0.7\textwidth}
\begin{tabular}{l|rr|rrrrrr}
\toprule
 & \multicolumn{2}{c|}{Perplexity} &  \multicolumn{6}{c}{Caption Length}\\
 & \multicolumn{2}{c|}{Text-Image} &  \multicolumn{2}{c}{Text} &  \multicolumn{2}{c}{Image} &  \multicolumn{2}{c}{Group}\\
 Model      &   Corr. &   p-value & Corr. &   p-value & Corr. &   p-value & Corr. &   p-value\\\midrule 
  MTurk Human                  & 0.05           & 0.07          & \textbf{0.11}  & \textbf{0.03} & \textbf{0.20}  & \textbf{0.00} & \textbf{0.20}  & \textbf{0.00} \\
 VinVL                        & \textbf{-0.05} & \textbf{0.04} & \textbf{-0.11} & \textbf{0.03} & \textbf{-0.18} & \textbf{0.00} & \textbf{-0.20} & \textbf{0.00} \\
 UNITER$_{large}$             & -0.01          & 0.57          & -0.08          & 0.13          & -0.06          & 0.20          & \textbf{-0.16} & \textbf{0.00} \\
 UNITER$_{base}$              & -0.03          & 0.22          & \textbf{-0.15} & \textbf{0.00} & \textbf{-0.11} & \textbf{0.03} & \textbf{-0.14} & \textbf{0.00} \\
 ViLLA$_{large}$              & -0.02          & 0.39          & -0.05          & 0.32          & \textbf{-0.13} & \textbf{0.01} & \textbf{-0.12} & \textbf{0.01} \\
 ViLLA$_{base}$               & -0.04          & 0.13          & \textbf{-0.14} & \textbf{0.01} & \textbf{-0.12} & \textbf{0.01} & \textbf{-0.11} & \textbf{0.03} \\
 VisualBERT$_{base}$          & -0.04          & 0.15          & -0.09          & 0.07          & -0.07          & 0.14          & -0.06          & 0.22          \\
 ViLT (ViT-B/32)              & -0.04          & 0.16          & -0.09          & 0.06          & \textbf{-0.20} & \textbf{0.00} & \textbf{-0.16} & \textbf{0.00} \\
 LXMERT                       & -0.04          & 0.12          & -0.00          & 0.97          & -0.05          & 0.32          & \textbf{-0.11} & \textbf{0.02} \\
 ViLBERT$_{base}$             & -0.04          & 0.11          & -0.09          & 0.09          & \textbf{-0.15} & \textbf{0.00} & \textbf{-0.14} & \textbf{0.00} \\
 UniT$_{ITM Finetuned}$       & -0.01          & 0.73          & -0.03          & 0.53          & -0.05          & 0.32          & -0.02          & 0.73          \\
 FLAVA$_{ITM}$                & -0.03          & 0.22          & \textbf{-0.21} & \textbf{0.00} & \textbf{-0.22} & \textbf{0.00} & \textbf{-0.23} & \textbf{0.00} \\
 FLAVA$_{ITC}$        & \textbf{-0.06} & \textbf{0.01} & \textbf{-0.15} & \textbf{0.00} & \textbf{-0.25} & \textbf{0.00} & \textbf{-0.19} & \textbf{0.00} \\
 CLIP (ViT-B/32)              & -0.04          & 0.09          & \textbf{-0.27} & \textbf{0.00} & \textbf{-0.19} & \textbf{0.00} & \textbf{-0.22} & \textbf{0.00} \\
 VSE++$_{COCO}$ (ResNet)      & \textbf{-0.05} & \textbf{0.04} & -0.03          & 0.60          & -0.02          & 0.74          & 0.01           & 0.90          \\
 VSE++$_{COCO}$ (VGG)         & -0.04          & 0.08          & -0.02          & 0.65          & 0.03           & 0.50          & 0.03           & 0.56          \\
 VSE++$_{Flickr30k}$ (ResNet) & -0.02          & 0.43          & -0.01          & 0.80          & 0.01           & 0.91          & 0.02           & 0.67          \\
 VSE++$_{Flickr30k}$ (VGG)    & 0.01           & 0.74          & -0.09          & 0.07          & -0.07          & 0.18          & \textbf{-0.10} & \textbf{0.04} \\
 VSRN$_{COCO}$                & \textbf{-0.07} & \textbf{0.01} & -0.03          & 0.60          & -0.05          & 0.30          & -0.05          & 0.36          \\
 VSRN$_{Flickr30k}$           & -0.02          & 0.32          & -0.03          & 0.60          & -0.10          & 0.06          & -0.05          & 0.29          \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{(left) The correlation between model image-caption scores and the caption perplexity from GPT2. (right) The correlation between the model text, image and group scores and the caption length.}
\label{tab:perplexity-and-length-correlations-baseline}
\end{table}

\paragraph{Ours.}

We shows correlations between caption perplexity and our model scores in \cref{tab:perplexity-and-length-correlations-ours}. Similar to previous results, for most models there is a weak correlation. Interestingly, most configurations of the best performing model (BLIP) show some correlation between caption perplexity and image-caption scores. Regarding caption length, there is a significant correlation for most models.

\begin{table}[ht]
\centering
\begin{adjustbox}{max width=0.7\textwidth}
\begin{tabular}{l|rr|rrrrrr}
\toprule
 & \multicolumn{2}{c|}{Perplexity} &  \multicolumn{6}{c}{Caption Length}\\
 & \multicolumn{2}{c|}{Image-Caption} &  \multicolumn{2}{c}{Text} &  \multicolumn{2}{c}{Image} &  \multicolumn{2}{c}{Group}\\
 Model      &   Corr. &   p-value & Corr. &   p-value & Corr. &   p-value & Corr. &   p-value\\\midrule 
MTurk Human                         & 0.05           & 0.07          & \textbf{0.11}  & \textbf{0.03} & \textbf{0.20}  & \textbf{0.00} & \textbf{0.20}  & \textbf{0.00} \\
 ViLT (ViT-B/32)                     & -0.04          & 0.08          & \textbf{-0.12} & \textbf{0.02} & -0.07          & 0.17          & -0.05          & 0.35          \\
 ViLT$_{COCO}$ (ViT-B/32)            & -0.05          & 0.06          & \textbf{-0.21} & \textbf{0.00} & \textbf{-0.16} & \textbf{0.00} & \textbf{-0.17} & \textbf{0.00} \\
 ViLT$_{Flickr30k}$ (ViT-B/32)       & \textbf{-0.05} & \textbf{0.03} & \textbf{-0.11} & \textbf{0.03} & \textbf{-0.17} & \textbf{0.00} & \textbf{-0.14} & \textbf{0.01} \\
 ViLT$_{NLVR2}$ (ViT-B/32)           & 0.00           & 0.95          & \textbf{-0.13} & \textbf{0.01} & \textbf{-0.11} & \textbf{0.03} & \textbf{-0.12} & \textbf{0.02} \\
 ViLT$_{VSR}$ Random (ViT-B/32)      & \textbf{-0.07} & \textbf{0.01} & \textbf{-0.16} & \textbf{0.00} & \textbf{-0.19} & \textbf{0.00} & \textbf{-0.15} & \textbf{0.00} \\
 ViLT$_{VSR}$ Zero-shot (ViT-B/32)   & \textbf{-0.08} & \textbf{0.00} & \textbf{-0.14} & \textbf{0.01} & \textbf{-0.15} & \textbf{0.00} & \textbf{-0.15} & \textbf{0.00} \\
 FLAVA$_{ITM}$                       & -0.03          & 0.22          & \textbf{-0.21} & \textbf{0.00} & \textbf{-0.22} & \textbf{0.00} & \textbf{-0.23} & \textbf{0.00} \\
 FLAVA$_{ITC}$                       & \textbf{-0.06} & \textbf{0.01} & \textbf{-0.15} & \textbf{0.00} & \textbf{-0.25} & \textbf{0.00} & \textbf{-0.19} & \textbf{0.00} \\
 CLIP (ViT-B/32)                     & -0.04          & 0.10          & \textbf{-0.28} & \textbf{0.00} & \textbf{-0.21} & \textbf{0.00} & \textbf{-0.23} & \textbf{0.00} \\
 CLIP (ViT-B/16)                     & -0.04          & 0.11          & \textbf{-0.26} & \textbf{0.00} & \textbf{-0.22} & \textbf{0.00} & \textbf{-0.23} & \textbf{0.00} \\
 CLIP (ViT-L/14)                     & -0.03          & 0.22          & \textbf{-0.22} & \textbf{0.00} & \textbf{-0.17} & \textbf{0.00} & \textbf{-0.18} & \textbf{0.00} \\
 CLIP (ViT-L/14-336)                 & -0.04          & 0.11          & \textbf{-0.23} & \textbf{0.00} & \textbf{-0.22} & \textbf{0.00} & \textbf{-0.23} & \textbf{0.00} \\
 OpenCLIP (ViT-B/32)            & -0.04          & 0.16          & \textbf{-0.13} & \textbf{0.01} & \textbf{-0.16} & \textbf{0.00} & \textbf{-0.18} & \textbf{0.00} \\
 OpenCLIP (ViT-L/14)            & -0.04          & 0.14          & \textbf{-0.22} & \textbf{0.00} & \textbf{-0.23} & \textbf{0.00} & \textbf{-0.23} & \textbf{0.00} \\
 OpenCLIP (ViT-H/14)            & -0.04          & 0.13          & \textbf{-0.22} & \textbf{0.00} & \textbf{-0.26} & \textbf{0.00} & \textbf{-0.24} & \textbf{0.00} \\
 OpenCLIP (ViT-g/14)            & -0.04          & 0.07          & \textbf{-0.26} & \textbf{0.00} & \textbf{-0.21} & \textbf{0.00} & \textbf{-0.20} & \textbf{0.00} \\
 OFA$_{Tiny}$                        & -0.01          & 0.66          & \textbf{-0.17} & \textbf{0.00} & -0.06          & 0.24          & \textbf{-0.12} & \textbf{0.02} \\
 OFA$_{Base}$                        & -0.02          & 0.43          & \textbf{-0.15} & \textbf{0.00} & \textbf{-0.12} & \textbf{0.02} & \textbf{-0.10} & \textbf{0.05} \\
 OFA$_{Medium}$                      & -0.01          & 0.77          & \textbf{-0.11} & \textbf{0.03} & \textbf{-0.14} & \textbf{0.00} & \textbf{-0.12} & \textbf{0.01} \\
 OFA$_{Large}$                       & \textbf{-0.16} & \textbf{0.00} & \textbf{-0.18} & \textbf{0.00} & \textbf{-0.20} & \textbf{0.00} & \textbf{-0.17} & \textbf{0.00} \\
 OFA$_{Huge}$                        & 0.01           & 0.75          & \textbf{-0.15} & \textbf{0.00} & \textbf{-0.17} & \textbf{0.00} & \textbf{-0.16} & \textbf{0.00} \\
 BLIP$_{ITM 14M}$ (ViT-B/16)         & -0.00          & 0.85          & \textbf{-0.22} & \textbf{0.00} & \textbf{-0.23} & \textbf{0.00} & \textbf{-0.21} & \textbf{0.00} \\
 BLIP$_{ITC 14M}$ (ViT-B/16)         & -0.00          & 0.97          & \textbf{-0.24} & \textbf{0.00} & \textbf{-0.17} & \textbf{0.00} & \textbf{-0.17} & \textbf{0.00} \\
 BLIP$_{ITM}$ (ViT-B/16)             & \textbf{-0.05} & \textbf{0.04} & \textbf{-0.24} & \textbf{0.00} & \textbf{-0.23} & \textbf{0.00} & \textbf{-0.22} & \textbf{0.00} \\
 BLIP$_{ITC}$ (ViT-B/16)             & \textbf{-0.06} & \textbf{0.02} & \textbf{-0.19} & \textbf{0.00} & \textbf{-0.17} & \textbf{0.00} & \textbf{-0.13} & \textbf{0.01} \\
 BLIP$_{ITM}$ (ViT-B/16) (CapFilt-L) & \textbf{-0.10} & \textbf{0.00} & \textbf{-0.20} & \textbf{0.00} & \textbf{-0.28} & \textbf{0.00} & \textbf{-0.23} & \textbf{0.00} \\
 BLIP$_{ITC}$ (ViT-B/16) (CapFilt-L) & \textbf{-0.10} & \textbf{0.00} & \textbf{-0.25} & \textbf{0.00} & \textbf{-0.17} & \textbf{0.00} & \textbf{-0.15} & \textbf{0.00} \\
 BLIP$_{ITM}$ (ViT-L/16)             & \textbf{-0.07} & \textbf{0.01} & \textbf{-0.17} & \textbf{0.00} & \textbf{-0.21} & \textbf{0.00} & \textbf{-0.19} & \textbf{0.00} \\
 BLIP$_{ITC}$ (ViT-L/16)             & \textbf{-0.08} & \textbf{0.00} & \textbf{-0.22} & \textbf{0.00} & \textbf{-0.17} & \textbf{0.00} & \textbf{-0.17} & \textbf{0.00} \\
 BLIP$_{ITM COCO}$ (ViT-B/16)        & -0.04          & 0.11          & \textbf{-0.17} & \textbf{0.00} & \textbf{-0.26} & \textbf{0.00} & \textbf{-0.22} & \textbf{0.00} \\
 BLIP$_{ITC COCO}$ (ViT-B/16)        & \textbf{-0.06} & \textbf{0.02} & \textbf{-0.18} & \textbf{0.00} & \textbf{-0.26} & \textbf{0.00} & \textbf{-0.22} & \textbf{0.00} \\
 BLIP$_{ITM Flickr30k}$ (ViT-B/16)   & -0.04          & 0.11          & \textbf{-0.25} & \textbf{0.00} & \textbf{-0.28} & \textbf{0.00} & \textbf{-0.28} & \textbf{0.00} \\
 BLIP$_{ITC Flickr30k}$ (ViT-B/16)   & \textbf{-0.07} & \textbf{0.00} & \textbf{-0.20} & \textbf{0.00} & \textbf{-0.19} & \textbf{0.00} & \textbf{-0.18} & \textbf{0.00} \\
 BLIP$_{ITM COCO}$ (ViT-L/16)        & \textbf{-0.06} & \textbf{0.02} & \textbf{-0.24} & \textbf{0.00} & \textbf{-0.23} & \textbf{0.00} & \textbf{-0.23} & \textbf{0.00} \\
 BLIP$_{ITC COCO}$ (ViT-L/16)        & \textbf{-0.10} & \textbf{0.00} & \textbf{-0.21} & \textbf{0.00} & \textbf{-0.21} & \textbf{0.00} & \textbf{-0.21} & \textbf{0.00} \\
 BLIP$_{ITM Flickr30k}$ (ViT-L/16)   & \textbf{-0.05} & \textbf{0.04} & \textbf{-0.27} & \textbf{0.00} & \textbf{-0.25} & \textbf{0.00} & \textbf{-0.23} & \textbf{0.00} \\
 BLIP$_{ITC Flickr30k}$ (ViT-L/16)   & \textbf{-0.09} & \textbf{0.00} & \textbf{-0.24} & \textbf{0.00} & \textbf{-0.19} & \textbf{0.00} & \textbf{-0.16} & \textbf{0.00} \\
 BLIP$_{NLVR2}$ (ViT-B/16)           & -0.00          & 0.85          & \textbf{-0.22} & \textbf{0.00} & -0.08          & 0.09          & \textbf{-0.16} & \textbf{0.00} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{(left) The correlation between model image-caption scores and the caption perplexity from GPT2. (right) The correlation between the model text, image and group scores and the caption length.}
\label{tab:perplexity-and-length-correlations-ours}
\end{table}

\section{Pretraining Dataset Size} \label{sec:dataset_size}

\paragraph{Previous.}

Winoground authors found highly significant correlations between the size of the multimodal pretraining dataset and the scores, if CLIP and FLAVA were removed as outliers. \cref{tab:data-size-correlations-baseline} shows these correlations, and \cref{fig:pretraining_images_baseline} has graphs showing each model's score versus the pretraining data size.

\begin{table}[ht]
\centering
\begin{tabular}{llrr}
\toprule
Pretraining & Score & Corr. & p-value\\\midrule
 & Text & \textbf{0.84} & \textbf{0.00} \\
 Image & Image & \textbf{0.76} & \textbf{0.00} \\
 & Group & \textbf{0.75} & \textbf{0.00} \\\midrule
 & Text  & \textbf{0.77} & \textbf{0.00} \\
 Caption & Image & \textbf{0.75} & \textbf{0.00} \\
 & Group & \textbf{0.71} & \textbf{0.00} \\
\bottomrule
\end{tabular}
\caption{Correlations between the number of pretraining images and captions and the model text, image, and group scores. CLIP and FLAVA are excluded as outliers.}
\label{tab:data-size-correlations-baseline}
\end{table}

\begin{figure}[ht]
    \centering
    \subfloat{\includegraphics[width=0.49\linewidth]{images/winoground/pretraining_images_baseline.png}}
    \subfloat{\includegraphics[width=0.49\linewidth]{images/winoground/pretraining_captions_baseline.png}}
    \caption{Graphs of the model performance on Winoground for each model by the number of pretraining images (left) and pretraining captions (right).}
    \label{fig:pretraining_images_baseline}
\end{figure}

\paragraph{Ours.}

We do the same analysis excluding ViLT as an outlier and find that there is no significant correlation. The models we test are pre-trained on large datasets, and the best ones are not the ones that are trained on most data. For example, BLIP is the best model and uses much less data than OpenCLIP for pre-training. \cref{tab:data-size-correlations-ours} shows these correlations, and \cref{fig:pretraining_images_ours} has graphs showing each model's score versus the pretraining data size.

\begin{table}[ht]
\centering
\begin{tabular}{llrr}
\toprule
Pretraining & Score & Corr. & p-value\\\midrule
               & Text    &  -0.11 &      0.54 \\
 Image              & Image   &  -0.28 &      0.12 \\
               & Group   &  -0.24 &      0.18 \\
             & Text    &  -0.11 &      0.54 \\
 Caption            & Image   &  -0.28 &      0.12 \\
             & Group   &  -0.24 &      0.18 \\
\bottomrule
\end{tabular}
\caption{Correlations between the number of pretraining images and captions and the model text, image, and group scores. ViLT is excluded as outlier.}
\label{tab:data-size-correlations-ours}
\end{table}

\begin{figure}[ht]
    \centering
    \subfloat{\includegraphics[width=0.49\linewidth]{images/winoground/pretraining_images_ours.png}}
    \subfloat{\includegraphics[width=0.49\linewidth]{images/winoground/pretraining_captions_ours.png}}
    \caption{Graphs of the model performance on Winoground for each model by the number of pretraining images (left) and pretraining captions (right).}
    \label{fig:pretraining_images_ours}
\end{figure}