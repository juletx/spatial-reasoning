{"cells":[{"cell_type":"markdown","metadata":{"id":"QcvGP5f7_3nY"},"source":["# Install Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TeIhqu9__ZB2"},"outputs":[],"source":["!pip install transformers\n","!pip install datasets"]},{"cell_type":"markdown","metadata":{"id":"26CSt5Jk__ei"},"source":["# Load Winoground (enter a huggingface user token or it will fail)\n","\n","Easily get a token by logging into https://huggingface.co/, clicking on your user profile -> settings -> access tokens -> new token"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dyH4My1BAD0h"},"outputs":[],"source":["from datasets import load_dataset\n","auth_token = \"\"  # Replace with an auth token, which you can get from your huggingface account: Profile -> Settings -> Access Tokens -> New Token\n","winoground = load_dataset(\"facebook/winoground\", use_auth_token=auth_token)[\"test\"]"]},{"cell_type":"markdown","metadata":{"id":"xMBH7v1XBy1o"},"source":["# Load FLAVA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kXL6d8UaB2QN"},"outputs":[],"source":["from transformers import FlavaProcessor, FlavaForPreTraining\n","flava_model = FlavaForPreTraining.from_pretrained(\"facebook/flava-full\").eval().to(\"cuda\")\n","flava_processor = FlavaProcessor.from_pretrained(\"facebook/flava-full\")"]},{"cell_type":"markdown","metadata":{"id":"JM7MAnFUCM2E"},"source":["# Look at an example from Winoground and get the image-caption scores from FLAVA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xtkrSjfhCdv-"},"outputs":[],"source":["from matplotlib import pyplot as plt\n","import torch\n","\n","ax1 = plt.subplot(1, 3, 1)\n","ax1.title.set_text('image_0')\n","plt.imshow(winoground[155][\"image_0\"].convert(\"RGB\"))\n","\n","ax2 = plt.subplot(1, 3, 2)\n","ax2.title.set_text('image_1')\n","plt.imshow(winoground[155][\"image_1\"].convert(\"RGB\"))\n","\n","plt.show()\n","\n","print(\"caption_0:\", winoground[155][\"caption_0\"])\n","print(\"caption_1:\", winoground[155][\"caption_1\"])\n","\n","# Note that some images in winoground are RGBA and some are RGB. Need to convert all to RGB with .convert('RGB')\n","inputs_c0_i0 = flava_processor(text=[winoground[155][\"caption_0\"]], images=[winoground[155][\"image_0\"].convert(\"RGB\")], return_tensors=\"pt\", max_length=77, padding=True, return_codebook_pixels=True, return_image_mask=True).to(\"cuda\")\n","inputs_c1_i0 = flava_processor(text=[winoground[155][\"caption_1\"]], images=[winoground[155][\"image_0\"].convert(\"RGB\")], return_tensors=\"pt\", max_length=77, padding=True, return_codebook_pixels=True, return_image_mask=True).to(\"cuda\")\n","inputs_c0_i1 = flava_processor(text=[winoground[155][\"caption_0\"]], images=[winoground[155][\"image_1\"].convert(\"RGB\")], return_tensors=\"pt\", max_length=77, padding=True, return_codebook_pixels=True, return_image_mask=True).to(\"cuda\")\n","inputs_c1_i1 = flava_processor(text=[winoground[155][\"caption_1\"]], images=[winoground[155][\"image_1\"].convert(\"RGB\")], return_tensors=\"pt\", max_length=77, padding=True, return_codebook_pixels=True, return_image_mask=True).to(\"cuda\")\n","\n","inputs_c0_i0[\"input_ids_masked\"] = inputs_c0_i0[\"input_ids\"].detach().clone() \n","inputs_c1_i0[\"input_ids_masked\"] = inputs_c1_i0[\"input_ids\"].detach().clone() \n","inputs_c0_i1[\"input_ids_masked\"] = inputs_c0_i1[\"input_ids\"].detach().clone() \n","inputs_c1_i1[\"input_ids_masked\"] = inputs_c1_i1[\"input_ids\"].detach().clone() \n","\n","inputs_c0_i0[\"bool_masked_pos\"] = torch.zeros_like(inputs_c0_i0[\"bool_masked_pos\"])\n","inputs_c1_i0[\"bool_masked_pos\"] = torch.zeros_like(inputs_c1_i0[\"bool_masked_pos\"])\n","inputs_c0_i1[\"bool_masked_pos\"] = torch.zeros_like(inputs_c0_i1[\"bool_masked_pos\"])\n","inputs_c1_i1[\"bool_masked_pos\"] = torch.zeros_like(inputs_c1_i1[\"bool_masked_pos\"])\n","\n","outputs_c0_i0 = flava_model(**inputs_c0_i0)\n","outputs_c1_i0 = flava_model(**inputs_c1_i0)\n","outputs_c0_i1 = flava_model(**inputs_c0_i1)\n","outputs_c1_i1 = flava_model(**inputs_c1_i1)\n","\n","flava_contrastive_scores_c0_i0 = outputs_c0_i0.contrastive_logits_per_image.item()\n","flava_contrastive_scores_c1_i0 = outputs_c1_i0.contrastive_logits_per_image.item()\n","flava_contrastive_scores_c0_i1 = outputs_c0_i1.contrastive_logits_per_image.item()\n","flava_contrastive_scores_c1_i1 = outputs_c1_i1.contrastive_logits_per_image.item()\n","print()\n","print(\"FLAVA contrastive image-text match scores:\")\n","print(\"image_0, caption_0:\", flava_contrastive_scores_c0_i0)\n","print(\"image_0, caption_1:\", flava_contrastive_scores_c1_i0)\n","print(\"image_1, caption_0:\", flava_contrastive_scores_c0_i1)\n","print(\"image_1, caption_1:\", flava_contrastive_scores_c1_i1)\n","\n","flava_itm_scores_c0_i0 = torch.nn.functional.softmax(outputs_c0_i0.itm_logits)[0][1].item()\n","flava_itm_scores_c1_i0 = torch.nn.functional.softmax(outputs_c1_i0.itm_logits)[0][1].item()\n","flava_itm_scores_c0_i1 = torch.nn.functional.softmax(outputs_c0_i1.itm_logits)[0][1].item()\n","flava_itm_scores_c1_i1 = torch.nn.functional.softmax(outputs_c1_i1.itm_logits)[0][1].item()\n","print()\n","print(\"FLAVA itm image-text match scores:\")\n","print(\"image_0, caption_0:\", flava_itm_scores_c0_i0)\n","print(\"image_0, caption_1:\", flava_itm_scores_c1_i0)\n","print(\"image_1, caption_0:\", flava_itm_scores_c0_i1)\n","print(\"image_1, caption_1:\", flava_itm_scores_c1_i1)"]},{"cell_type":"markdown","metadata":{"id":"hS35igYrGRfk"},"source":["# Get FLAVA image-caption scores from the whole dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QzxMKHOLGUnS"},"outputs":[],"source":["from tqdm import tqdm\n","winoground_flava_contrastive_scores = []\n","winoground_flava_itm_scores = []\n","for example in tqdm(winoground):\n","  # Note that some images in winoground are RGBA and some are RGB. Need to convert all to RGB with .convert('RGB')\n","  inputs_c0_i0 = flava_processor(text=[example[\"caption_0\"]], images=[example[\"image_0\"].convert(\"RGB\")], return_tensors=\"pt\", padding=\"max_length\", max_length=77, return_codebook_pixels=True, return_image_mask=True).to(\"cuda\")\n","  inputs_c1_i0 = flava_processor(text=[example[\"caption_1\"]], images=[example[\"image_0\"].convert(\"RGB\")], return_tensors=\"pt\", padding=\"max_length\", max_length=77, return_codebook_pixels=True, return_image_mask=True).to(\"cuda\")\n","  inputs_c0_i1 = flava_processor(text=[example[\"caption_0\"]], images=[example[\"image_1\"].convert(\"RGB\")], return_tensors=\"pt\", padding=\"max_length\", max_length=77, return_codebook_pixels=True, return_image_mask=True).to(\"cuda\")\n","  inputs_c1_i1 = flava_processor(text=[example[\"caption_1\"]], images=[example[\"image_1\"].convert(\"RGB\")], return_tensors=\"pt\", padding=\"max_length\", max_length=77, return_codebook_pixels=True, return_image_mask=True).to(\"cuda\")\n","  \n","  inputs_c0_i0[\"input_ids_masked\"] = inputs_c0_i0[\"input_ids\"].detach().clone() \n","  inputs_c1_i0[\"input_ids_masked\"] = inputs_c1_i0[\"input_ids\"].detach().clone() \n","  inputs_c0_i1[\"input_ids_masked\"] = inputs_c0_i1[\"input_ids\"].detach().clone() \n","  inputs_c1_i1[\"input_ids_masked\"] = inputs_c1_i1[\"input_ids\"].detach().clone() \n","\n","  inputs_c0_i0[\"bool_masked_pos\"] = torch.zeros_like(inputs_c0_i0[\"bool_masked_pos\"])\n","  inputs_c1_i0[\"bool_masked_pos\"] = torch.zeros_like(inputs_c1_i0[\"bool_masked_pos\"])\n","  inputs_c0_i1[\"bool_masked_pos\"] = torch.zeros_like(inputs_c0_i1[\"bool_masked_pos\"])\n","  inputs_c1_i1[\"bool_masked_pos\"] = torch.zeros_like(inputs_c1_i1[\"bool_masked_pos\"])\n","\n","  outputs_c0_i0 = flava_model(**inputs_c0_i0)\n","  outputs_c1_i0 = flava_model(**inputs_c1_i0)\n","  outputs_c0_i1 = flava_model(**inputs_c0_i1)\n","  outputs_c1_i1 = flava_model(**inputs_c1_i1)\n","\n","  flava_contrastive_scores_c0_i0 = outputs_c0_i0.contrastive_logits_per_image.item()\n","  flava_contrastive_scores_c1_i0 = outputs_c1_i0.contrastive_logits_per_image.item()\n","  flava_contrastive_scores_c0_i1 = outputs_c0_i1.contrastive_logits_per_image.item()\n","  flava_contrastive_scores_c1_i1 = outputs_c1_i1.contrastive_logits_per_image.item()\n","  winoground_flava_contrastive_scores.append({\"id\" : example[\"id\"], \"c0_i0\": flava_contrastive_scores_c0_i0, \"c0_i1\": flava_contrastive_scores_c0_i1, \"c1_i0\": flava_contrastive_scores_c1_i0, \"c1_i1\": flava_contrastive_scores_c1_i1})\n","\n","  flava_itm_scores_c0_i0 = torch.nn.functional.softmax(outputs_c0_i0.itm_logits)[0][1].item()\n","  flava_itm_scores_c1_i0 = torch.nn.functional.softmax(outputs_c1_i0.itm_logits)[0][1].item()\n","  flava_itm_scores_c0_i1 = torch.nn.functional.softmax(outputs_c0_i1.itm_logits)[0][1].item()\n","  flava_itm_scores_c1_i1 = torch.nn.functional.softmax(outputs_c1_i1.itm_logits)[0][1].item()\n","  winoground_flava_itm_scores.append({\"id\" : example[\"id\"], \"c0_i0\": flava_itm_scores_c0_i0, \"c0_i1\": flava_itm_scores_c0_i1, \"c1_i0\": flava_itm_scores_c1_i0, \"c1_i1\": flava_itm_scores_c1_i1})"]},{"cell_type":"markdown","metadata":{"id":"0vW99Wb8IR86"},"source":["# Define the text, image, and group metrics, and compute the overall performance of FLAVA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bOsR6mn1IXB_"},"outputs":[],"source":["def text_correct(result):\n","    return result[\"c0_i0\"] > result[\"c1_i0\"] and result[\"c1_i1\"] > result[\"c0_i1\"]\n","\n","def image_correct(result):\n","    return result[\"c0_i0\"] > result[\"c0_i1\"] and result[\"c1_i1\"] > result[\"c1_i0\"]\n","\n","def group_correct(result):\n","    return image_correct(result) and text_correct(result)\n","\n","\n","contrastive_text_correct_count = 0\n","contrastive_image_correct_count = 0\n","contrastive_group_correct_count = 0\n","for result in winoground_flava_contrastive_scores:\n","  contrastive_text_correct_count += 1 if text_correct(result) else 0\n","  contrastive_image_correct_count += 1 if image_correct(result) else 0\n","  contrastive_group_correct_count += 1 if group_correct(result) else 0\n","\n","denominator = len(winoground_flava_contrastive_scores)\n","print(\"contrastive text score:\", contrastive_text_correct_count/denominator)\n","print(\"contrastive image score:\", contrastive_image_correct_count/denominator)\n","print(\"contrastive group score:\", contrastive_group_correct_count/denominator)\n","\n","itm_text_correct_count = 0\n","itm_image_correct_count = 0\n","itm_group_correct_count = 0\n","for result in winoground_flava_itm_scores:\n","  itm_text_correct_count += 1 if text_correct(result) else 0\n","  itm_image_correct_count += 1 if image_correct(result) else 0\n","  itm_group_correct_count += 1 if group_correct(result) else 0\n","\n","denominator = len(winoground_flava_itm_scores)\n","print(\"itm text score:\", itm_text_correct_count/denominator)\n","print(\"itm image score:\", itm_image_correct_count/denominator)\n","print(\"itm group score:\", itm_group_correct_count/denominator)\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"winoground-flava-example.ipynb","provenance":[{"file_id":"1c3l4r4cEA5oXfq9uXhrJibddwRkcBxzP","timestamp":1657906780994},{"file_id":"15wwOSte2CjTazdnCWYUm2VPlFbk2NGc0","timestamp":1652465420795}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}